# -*- coding: utf-8 -*-
"""Evaluator for clustering experiments."""
import ast
from typing import Dict, List, Tuple, Union

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    mutual_info_score,
    normalized_mutual_info_score,
    rand_score,
)

from sktime.benchmarking.evaluators.base import BaseEstimatorEvaluator
from sktime.clustering.metrics import silhouette_score
from sktime.datasets import load_from_tsfile, read_clusterer_result_from_uea_format
from sktime.datatypes import convert_to

_evaluation_metrics_dict = {
    "RI": rand_score,  # Rand index
    "ARI": adjusted_rand_score,  # Adjusted rand index
    "MI": mutual_info_score,  # Mutual information
    "NMI": normalized_mutual_info_score,  # Normalized mutual information
    "AMI": adjusted_mutual_info_score,  # Adjusted mutual information
    "SC": silhouette_score,  # Silhouette Coefficient
    "ACC": accuracy_score,  # Accuracy
}

_metric_require_dataset = ["SC"]


class ClusterEvaluator(BaseEstimatorEvaluator):
    """Evaluator for clustering experiments.

    Parameters
    ----------
    results_path: str
        The path to the csv results generated by the estimators.
    evaluation_out_path: str
        The path to output the results of the evaluation
    experiment_name: str
        The name of the experiment (the directory that stores the results will be called
        this).
    metrics: List[str], defaults = None
        List of metrics to evaluate over. If None is specified all metrics that don't
        require a dataset path are used. If you want to easily use all the metrics
        specify 'metrics = "all"'.
    naming_parameter_key: str, defaults = None
        The key from the dict defining the parameters of the experiments (first line of
        csv file).
    draw_critical_difference_diagrams: bool, defaults = True
        Boolean that when true will also output critical difference diagrams. When
        False no diagrams will be outputted.
    critical_diff_params: dict, defaults = None
        Parameters for critical difference call. See create_critical_difference_diagram
        method for list of valid parameters.
    dataset_paths: str, defaults = None
        Dict containing the path or values each dataset. The key should be the dataset
        name str and the value should be a list with either the paths or a np.ndarray
        which are the datasets. A mixture of paths and np.ndarray can be passed. It is
        assumed that the dataset path points to a .ts file. The first value in the list
        should be training data and the second should be the test. For example:
        {
            'dataset1' : ['training_path/dataset1.ts', testing_path/dataset1.ts']
            'dataset2' : [np.array([[1,2,3]...]), 'testing_path/dataset2.ts']
            'dataset3' : ['data/dataset3']
        }
        If there is no split then the first value is taken and assumed as the whole
        dataset.
    """

    def __init__(
        self,
        results_path: str,
        evaluation_out_path: str,
        experiment_name: str,
        metrics: Union[str, List[str]] = None,
        naming_parameter_key: str = None,
        draw_critical_difference_diagrams: bool = True,
        critical_diff_params: dict = None,
        dataset_paths: dict = None,
    ):
        if metrics is None:
            metrics = ["RI", "ARI", "MI", "NMI", "AMI", "ACC"]
        if metrics == "all":
            metrics = list(_evaluation_metrics_dict.keys())
        for metric in metrics:
            if metric not in _evaluation_metrics_dict:
                raise ValueError(
                    f"The metric: {metric} is invalid please check the "
                    f"list of available metrics to use."
                )
        self.metrics = metrics
        self.naming_parameter_key = naming_parameter_key

        self.dataset_paths = dataset_paths
        if dataset_paths is None:
            self.dataset_paths = {}

        super(ClusterEvaluator, self).__init__(
            results_path,
            evaluation_out_path,
            experiment_name,
            draw_critical_difference_diagrams=draw_critical_difference_diagrams,
            critical_diff_params=critical_diff_params,
        )

    def evaluate_csv_data(self, csv_path: str) -> Tuple:
        """Evaluate results from csv file.

        Parameters
        ----------
        csv_path: str
            Path to csv containing the results to analyse.

        Returns
        -------
        dataset: str
            Dataset for the experiment.
        estimator_name: str
            Estimator name.
        experiment_name: str
            Name for experiment (maybe the same as the estimator name).
        metric_scores: dict
            Dict where the key is the metric and the value is the score.
        """
        data = read_clusterer_result_from_uea_format(csv_path)
        first_line = data["first_line_comment"]
        parameters = ast.literal_eval(",".join((data["estimator_parameters"])))
        temp = pd.DataFrame(data["predictions"])
        predictions_df = pd.DataFrame(temp[[0, 1]])
        metrics_score = self._compute_metrics(
            predictions_df, first_line[0], first_line[2]
        )
        estimator_name = self.get_estimator_name(first_line, parameters)
        return (first_line[0], first_line[1], estimator_name, metrics_score)

    def get_estimator_name(
        self, estimator_details: List[str], estimator_params: List[str]
    ) -> str:
        """Generate estimator name from parameters.

        Parameters
        ----------
        estimator_details: List[str]
            List of strings containing details about the estimator.
        estimator_params: List[str]
            List of strings containing details of the parameters the estimator used.

        Returns
        -------
        str
            Name of estimator to use.
        """
        estimator_name = estimator_details[1]
        if self.naming_parameter_key is not None:
            estimator_name = (
                f"{estimator_name}-{estimator_params[self.naming_parameter_key]}"
            )
        return estimator_name

    def _compute_metrics(
        self, predictions_df: pd.DataFrame, dataset_name: str, split: str
    ) -> Dict:
        """Compute the metrics for a given dataset.

        Parameters
        ----------
        predictions_df: pd.DataFrame
            Dataframe containing two columns. The first column is 'True y class' which
            is the actual class and the second column is the 'Predicted y class' which
            is the estimators prediction.
        dataset_name: str
            Dataset the metric is taken over.
        split: str
            Data split which is either 'train' or 'test'

        Returns
        -------
        dict
            Dict where the key is the metric and the value is the score for that
            metric.
        """
        numpy_pred = predictions_df.to_numpy()
        true_class = [i[0] for i in numpy_pred[1:]]
        predicted_class = [i[1] for i in numpy_pred[1:]]
        metric_scores = {}
        for metric in self.metrics:
            metric_callable = _evaluation_metrics_dict[metric]
            if metric in _metric_require_dataset:
                if self.dataset_paths == {}:
                    raise ValueError(
                        f"Unable to evaluate over metric {metric} as a "
                        f"dataset is required. Please specify the dataset "
                        f"or path to the dataset in the constructors "
                        f"'dataset_paths' parameter."
                    )
                curr_dataset = self.dataset_paths[dataset_name]
                if isinstance(curr_dataset, list) and len(curr_dataset) > 1:
                    index = 0
                    if split == "test":
                        index = 1
                    curr_dataset = curr_dataset[index]
                X = None
                if isinstance(curr_dataset, np.ndarray):
                    X = curr_dataset
                elif isinstance(curr_dataset, str):
                    X = convert_to(load_from_tsfile(curr_dataset), "numpy3D")

                if X is None:
                    raise ValueError(
                        f"The value for the dataset {dataset_name} in the "
                        f"dataset_paths parameter is not a str or "
                        f"np.ndarray so unable to process."
                    )
                metric_scores[metric] = metric_callable(X, predicted_class)
            else:
                metric_scores[metric] = metric_callable(true_class, predicted_class)

        return metric_scores


if __name__ == "__main__":
    from sktime.datasets import load_acsf1, load_arrow_head, load_osuleaf

    datasets = [
        ("acsf1", load_acsf1),
        ("arrowhead", load_arrow_head),
        ("osuleaf", load_osuleaf),
    ]

    dataset_paths = {}

    for dataset in datasets:
        dataset_name = dataset[0]
        load_dataset = dataset[1]
        X_train, y_train = load_dataset(split="train")
        X_test, y_test = load_dataset(split="test")

        num_data_train = len(X_train)
        num_data_test = len(X_test)

        max_num_data_points = 20

        if num_data_train > max_num_data_points:
            num_data_train = max_num_data_points

        if num_data_test > max_num_data_points:
            num_data_test = max_num_data_points

        trainX = convert_to(X_train[0:num_data_train], "numpy3D")
        testX = convert_to(X_test[0:num_data_test], "numpy3D")
        dataset_paths[dataset_name] = [trainX, testX]

    evaluator = ClusterEvaluator(
        "/sktime/benchmarking/evaluation/tests/test_results",
        "C:\\Users\\chris\\Documents\\Projects\\sktime\\sktime\\benchmarking"
        "\\evaluation\\tests\\result_out",
        experiment_name="example_experiment",
        naming_parameter_key="metric",
        critical_diff_params={"alpha": 100000.0},
        metrics="all",
        dataset_paths=dataset_paths,
    )

    evaluator.run_evaluation(["kmeans", "kmedoids"])
